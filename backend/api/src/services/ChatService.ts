/**
 * ============================================================
 * ChatService - AI ì±„íŒ… ì„œë¹„ìŠ¤ ëª¨ë“ˆ
 * ============================================================
 * 
 * LLMì„ í†µí•œ ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
 * 
 * @module services/ChatService
 * @description
 * - ì—ì´ì „íŠ¸ ìë™ ë¼ìš°íŒ… ë° ì„ íƒ
 * - ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… ë° ë¶„ì„
 * - MCP ë„êµ¬ ì‹¤í–‰ (Agent Loop)
 * - ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ëª¨ë“œ
 * - API ì‚¬ìš©ëŸ‰ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§
 */

import { OllamaClient } from '../ollama/client';
import { routeToAgent, getAgentSystemMessage, AGENTS } from '../agents';
import { getPromptConfig } from '../chat/prompt';
import { getSequentialThinkingServer, applySequentialThinking } from '../mcp/sequential-thinking';
import { getGptOssTaskPreset, isGeminiModel, ModelOptions } from '../ollama/types';
import { selectOptimalModel, adjustOptionsForModel, checkModelCapability, ModelSelection } from '../chat/model-selector';
import { ExecutionPlan } from '../chat/profile-resolver';
import { DocumentResult } from '../documents/processor';
import { DocumentStore } from '../documents/store';
import { createDiscussionEngine, DiscussionProgress, DiscussionResult } from '../agents/discussion-engine';
import { DeepResearchService, ResearchProgress } from './DeepResearchService';
import { getUnifiedDatabase } from '../data/models/unified-database';
import { v4 as uuidv4 } from 'uuid';
import { getApiUsageTracker } from '../ollama/api-usage-tracker';
import { getApiKeyManager } from '../ollama/api-key-manager';
import { ToolDefinition, ChatMessage } from '../ollama/types';
import { UserTier } from '../data/user-manager';
import { canUseTool } from '../mcp/tool-tiers';
import { getUnifiedMCPClient } from '../mcp/unified-client';
import { UserContext } from '../mcp/user-sandbox';

// ============================================================
// A2A Multi-Model Parallel Configuration
// ============================================================

/** A2A ë³‘ë ¬ ì‘ë‹µì— ì‚¬ìš©í•  ëª¨ë¸ ì„¤ì • */
const A2A_MODELS = {
    primary: 'gpt-oss:120b-cloud',
    secondary: 'gemini-3-flash-preview:cloud',
    synthesizer: 'gemini-3-flash-preview:cloud',
} as const;

/** A2A ì¢…í•© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ â€” ë‘ ëª¨ë¸ì˜ ì‘ë‹µì„ í•˜ë‚˜ë¡œ í•©ì„± */
const A2A_SYNTHESIS_SYSTEM_PROMPT = [
    'ë‹¹ì‹ ì€ ë‘ AI ëª¨ë¸ì˜ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ìµœê³  í’ˆì§ˆì˜ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.',
    '',
    '## ì¢…í•© ì§€ì¹¨',
    '1. ê° ì‘ë‹µì—ì„œ ê°€ì¥ ê°•ë ¥í•˜ê³  ì •í™•í•œ í¬ì¸íŠ¸ë¥¼ ì‹ë³„í•˜ì„¸ìš”.',
    '2. ëª¨ìˆœë˜ëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´ ë” ì •í™•í•˜ê³  ìƒì„¸í•œ ìª½ì„ ì±„íƒí•˜ì„¸ìš”.',
    '3. ì–‘ìª½ì˜ ë³´ì™„ì  ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ê²°í•©í•˜ì„¸ìš”.',
    '4. ì½”ë“œ ë¸”ë¡, ë§ˆí¬ë‹¤ìš´ ì„œì‹, êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ì„¸ìš”.',
    '5. ì›ë³¸ ì§ˆë¬¸ê³¼ ë™ì¼í•œ ì–¸ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.',
    '',
    '## ì¶œë ¥ í˜•ì‹',
    'ìµœì¢… ì¢…í•© ë‹µë³€ë§Œ ì¶œë ¥í•˜ì„¸ìš”. "ëª¨ë¸ Aì— ë”°ë¥´ë©´..." ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.',
].join('\n');

/**
 * Chat message structure for conversation history
 * Uses Record<string, unknown> for flexibility with existing code
 */
export interface ChatHistoryMessage {
    role: 'user' | 'assistant' | 'system' | 'tool';
    content: string;
    images?: string[];
    tool_calls?: Array<{
        type?: string;
        function: {
            name: string;
            arguments: Record<string, unknown> | string;
        };
    }>;
    [key: string]: unknown;
}

/**
 * Agent information for selection callback
 */
export interface AgentSelectionInfo {
    type?: string;
    name?: string;
    emoji?: string;
    phase?: string;
    reason?: string;
    confidence?: number;
    [key: string]: unknown;
}

/**
 * Tool call structure from LLM response
 */
export interface ToolCallInfo {
    type?: string;
    function: {
        name: string;
        arguments: Record<string, unknown>;
    };
}

/**
 * Web search result type
 */
export interface WebSearchResult {
    title: string;
    url: string;
    snippet?: string;
}

/**
 * Web search function type
 */
export type WebSearchFunction = (
    query: string,
    options?: { maxResults?: number }
) => Promise<WebSearchResult[]>;

/**
 * Chat metrics interface - flexible to accommodate various metric types
 */
/** Per-message response metadata (distinct from monitoring/metrics.ts ChatMetrics) */
export interface ChatResponseMeta {
    model?: string;
    tokens?: number;
    duration?: number;
    [key: string]: unknown;
}

/**
 * ì±„íŒ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì„¤ì •
 * @interface ChatServiceConfig
 */
export interface ChatServiceConfig {
    /** Ollama í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ */
    client: OllamaClient;
    /** ì‚¬ìš©í•  LLM ëª¨ë¸ëª… */
    model: string;
}

/**
 * ì±„íŒ… ë©”ì‹œì§€ ìš”ì²­ ì¸í„°í˜ì´ìŠ¤
 * @interface ChatMessageRequest
 */
export interface ChatMessageRequest {
    /** ì‚¬ìš©ì ë©”ì‹œì§€ ë‚´ìš© */
    message: string;
    /** ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒì ) */
    history?: Array<{ role: string; content: string; images?: string[] }>;
    /** ì°¸ì¡° ë¬¸ì„œ ID (ì„ íƒì ) */
    docId?: string;
    /** ì´ë¯¸ì§€ ë°ì´í„° ë°°ì—´ - base64 ì¸ì½”ë”© (ì„ íƒì ) */
    images?: string[];
    /** ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì ) */
    webSearchContext?: string;
    /** ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ */
    discussionMode?: boolean;
    /** ğŸ”¬ Deep Research ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ */
    deepResearchMode?: boolean;
    /** íŒ”ë¼ë§ˆ Native Thinking ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ */
    thinkingMode?: boolean;
    /** Thinking ë ˆë²¨ (low/medium/high) */
    thinkingLevel?: 'low' | 'medium' | 'high';
    /** ğŸ†• ì‚¬ìš©ì ID (ë©”ëª¨ë¦¬ ì„œë¹„ìŠ¤ ì—°ë™ìš©) */
    userId?: string;
    /** ğŸ†• ì‚¬ìš©ì ì—­í•  (admin/user/guest) - ë„êµ¬ ê¶Œí•œ ê²°ì •ì— ì‚¬ìš© */
    userRole?: 'admin' | 'user' | 'guest';
    /** ğŸ†• ì‚¬ìš©ì ë“±ê¸‰ (free/pro/enterprise) - ëª…ì‹œì  ì§€ì • ì‹œ ì‚¬ìš© */
    userTier?: UserTier;
    /** ğŸ†• ì¤‘ë‹¨ ì‹œê·¸ë„ - ì‚¬ìš©ìê°€ ì‘ë‹µ ìƒì„±ì„ ì¤‘ë‹¨í•  ë•Œ ì‚¬ìš© */
    abortSignal?: AbortSignal;
}

/**
 * AI ì±„íŒ… ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
 * 
 * LLMì„ í†µí•œ ë©”ì‹œì§€ ì²˜ë¦¬, ì—ì´ì „íŠ¸ ë¼ìš°íŒ…, ë„êµ¬ ì‹¤í–‰ ë“±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
 * 
 * @class ChatService
 * @example
 * const chatService = new ChatService(ollamaClient);
 * const response = await chatService.processMessage({
 *     message: 'ì•ˆë…•í•˜ì„¸ìš”',
 *     history: []
 * }, uploadedDocs, (token) => console.log(token));
 */
export class ChatService {
    /** Ollama LLM í´ë¼ì´ì–¸íŠ¸ */
    private client: OllamaClient;
    
    /** ğŸ†• í˜„ì¬ ìš”ì²­ì˜ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ (ë„êµ¬ ê¶Œí•œ ê²€ì¦ìš©) */
    private currentUserContext: UserContext | null = null;

    /**
     * ChatService ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
     * @param client - Ollama í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
     */
    constructor(client: OllamaClient) {
        this.client = client;
    }
    
    /**
     * ğŸ†• ì‚¬ìš©ì ì—­í• ì— ë”°ë¥¸ ë„êµ¬ ë“±ê¸‰ ê²°ì •
     * - admin â†’ enterprise (ëª¨ë“  ë„êµ¬ í—ˆìš©)
     * - user â†’ ëª…ì‹œëœ tier ë˜ëŠ” free
     * - guest â†’ free
     */
    private resolveUserTier(userRole?: 'admin' | 'user' | 'guest', explicitTier?: UserTier): UserTier {
        // adminì€ í•­ìƒ enterprise
        if (userRole === 'admin') {
            return 'enterprise';
        }
        
        // ëª…ì‹œì ìœ¼ë¡œ ì§€ì •ëœ tierê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if (explicitTier) {
            return explicitTier;
        }
        
        // ê¸°ë³¸ê°’: free
        return 'free';
    }
    
    /**
     * ğŸ†• í˜„ì¬ ìš”ì²­ì˜ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
     */
    private setUserContext(userId: string, userRole?: 'admin' | 'user' | 'guest', userTier?: UserTier): void {
        const tier = this.resolveUserTier(userRole, userTier);
        this.currentUserContext = {
            userId: userId || 'guest',
            tier,
            role: userRole || 'guest'
        };
        console.log(`[ChatService] ğŸ” ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì„¤ì •: userId=${userId}, role=${userRole}, tier=${tier}`);
    }

    /**
     * ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
     * 
     * ì²˜ë¦¬ íë¦„:
     * 1. í† ë¡  ëª¨ë“œ í™•ì¸ ë° ë¶„ê¸°
     * 2. ì—ì´ì „íŠ¸ ìë™ ì„ íƒ (LLM ê¸°ë°˜)
     * 3. ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
     * 4. Sequential Thinking ì ìš©
     * 5. Agent Loop - MCP ë„êµ¬ ì‹¤í–‰
     * 6. API ì‚¬ìš©ëŸ‰ ì¶”ì 
     * 
     * @param req - ì±„íŒ… ë©”ì‹œì§€ ìš”ì²­ ê°ì²´
     * @param uploadedDocuments - ì—…ë¡œë“œëœ ë¬¸ì„œ ë§µ
     * @param onToken - í† í° ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
     * @param onAgentSelected - ì—ì´ì „íŠ¸ ì„ íƒ ì•Œë¦¼ ì½œë°± (ì„ íƒì )
     * @param onDiscussionProgress - í† ë¡  ì§„í–‰ ìƒí™© ì½œë°± (ì„ íƒì )
     * @param onResearchProgress - ì‹¬ì¸µ ì—°êµ¬ ì§„í–‰ ìƒí™© ì½œë°± (ì„ íƒì )
     * @returns ìµœì¢… ì‘ë‹µ ë¬¸ìì—´
     */
    async processMessage(
        req: ChatMessageRequest,
        uploadedDocuments: DocumentStore,
        onToken: (token: string) => void,
        onAgentSelected?: (agent: { type: string; name: string; emoji?: string; phase?: string; reason?: string; confidence?: number }) => void,
        onDiscussionProgress?: (progress: DiscussionProgress) => void,
        onResearchProgress?: (progress: ResearchProgress) => void,
        executionPlan?: ExecutionPlan
    ): Promise<string> {
        const { message, history, docId, images, webSearchContext, discussionMode, deepResearchMode, thinkingMode, thinkingLevel, userId, userRole, userTier, abortSignal } = req;

        // ğŸ†• ì¤‘ë‹¨ ì²´í¬ í—¬í¼ í•¨ìˆ˜
        const checkAborted = () => {
            if (abortSignal?.aborted) {
                throw new Error('ABORTED');
            }
        };

        // ğŸ†• ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ë„êµ¬ ê¶Œí•œ ê²€ì¦ìš©)
        this.setUserContext(userId || 'guest', userRole, userTier);

        // ğŸ¯ í† ë¡  ëª¨ë“œ ì²˜ë¦¬
        if (discussionMode) {
            return this.processMessageWithDiscussion(req, uploadedDocuments, onToken, onDiscussionProgress);
        }

        // ğŸ”¬ Deep Research ëª¨ë“œ ì²˜ë¦¬
        if (deepResearchMode) {
            return this.processMessageWithDeepResearch(req, onToken, onResearchProgress);
        }

        const startTime = Date.now(); // ğŸ†• ì‘ë‹µ ì‹œê°„ ì¶”ì 
        let fullResponse = '';

        // ğŸš€ 1. ì—ì´ì „íŠ¸ ìë™ ì„ íƒ (LLM ê¸°ë°˜)
        const agentSelection = await routeToAgent(message || '');
        const agentSystemMessage = getAgentSystemMessage(agentSelection);
        const selectedAgent = AGENTS[agentSelection.primaryAgent];

        console.log(`[ChatService] ì—ì´ì „íŠ¸: ${selectedAgent.emoji} ${selectedAgent.name}`);

        // ì—ì´ì „íŠ¸ ì„ íƒ ì •ë³´ ì½œë°± í˜¸ì¶œ
        if (onAgentSelected && selectedAgent) {
            onAgentSelected({
                type: agentSelection.primaryAgent,
                name: selectedAgent.name,
                emoji: selectedAgent.emoji,
                phase: agentSelection.phase || 'planning',
                reason: agentSelection.reason || '',
                confidence: agentSelection.confidence || 0.5
            });
        }

        // ğŸ“„ 2. ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        let documentContext = '';
        let documentImages: string[] = [];

        if (docId) {
            const doc = uploadedDocuments.get(docId);
            if (doc) {
                // í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                let docText = doc.text || '';
                const maxChars = isGeminiModel(this.client.model) ? 100000 : 30000;

                if (docText.length > maxChars) {
                    const half = Math.floor(maxChars / 2);
                    const front = docText.substring(0, half);
                    const back = docText.substring(docText.length - half);
                    docText = `${front}\n\n... [ì¤‘ê°„ ë‚´ìš© ìƒëµ] ...\n\n${back}`;
                }

                documentContext = `## ğŸ“š REFERENCE DOCUMENT: ${doc.filename}\n` +
                    `Type: ${doc.type.toUpperCase()}\n` +
                    `Length: ${doc.text.length} chars\n\n` +
                    `CONTENT:\n---\n${docText}\n---\n\n` +
                    `Please analyze the document above and answer the user's question.\n\n`;

                // ë¹„ì „ ë°ì´í„° ì£¼ì…
                if (['image', 'pdf'].includes(doc.type) && doc.info?.base64) {
                    documentImages.push(doc.info.base64);
                }
            }
        }

        // ğŸ¤” 3. Sequential Thinking ì ìš© (ì‚¬ìš©ìê°€ Thinking Mode í™œì„±í™” ì‹œì—ë§Œ)
        const thinkingServer = getSequentialThinkingServer();
        // thinkingServer.reset(); // í•„ìš” ì‹œ ë¦¬ì…‹
        let enhancedUserMessage = applySequentialThinking(message, thinkingMode === true);

        // âœ‰ï¸ 4. ìµœì¢… ë©”ì‹œì§€ ì¡°ë¦½
        let finalEnhancedMessage = '';
        if (documentContext) finalEnhancedMessage += documentContext;
        if (webSearchContext) finalEnhancedMessage += webSearchContext;
        finalEnhancedMessage += `\n## USER QUESTION\n${enhancedUserMessage}`;

        // âš™ï¸ 5. í”„ë¡¬í”„íŠ¸ ë° ì˜µì…˜ ì„¤ì • + ğŸ†• ëª¨ë¸ ìë™ ì„ íƒ
        const promptConfig = getPromptConfig(message);
        
        // Â§9 Pipeline Profile: brand modelì´ë©´ í”„ë¡œíŒŒì¼ ì—”ì§„ ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ì¡´ ìë™ ì„ íƒ
        const hasImages = (images && images.length > 0) || documentImages.length > 0;
        let modelSelection: ModelSelection;
        
        if (executionPlan?.isBrandModel) {
            // Brand model â†’ í”„ë¡œíŒŒì¼ì˜ ì—”ì§„ ëª¨ë¸ ì‚¬ìš© (ìë™ ì„ íƒ ë°”ì´íŒ¨ìŠ¤)
            console.log(`[ChatService] Â§9 Brand Model: ${executionPlan.requestedModel} â†’ engine=${executionPlan.resolvedEngine}`);
            this.client.setModel(executionPlan.resolvedEngine);
            modelSelection = {
                model: executionPlan.resolvedEngine,
                options: promptConfig.options || {},
                reason: `Brand model ${executionPlan.requestedModel} â†’ ${executionPlan.resolvedEngine}`,
                queryType: 'chat',
                supportsToolCalling: true,
                supportsThinking: true,
                supportsVision: executionPlan.requiredTools.includes('vision'),
            };
        } else {
            // ê¸°ì¡´ ì§ˆë¬¸ ìœ í˜• ê¸°ë°˜ ìë™ ì„ íƒ
            modelSelection = selectOptimalModel(message, hasImages);
            console.log(`[ChatService] ğŸ¯ ëª¨ë¸ ìë™ ì„ íƒ: ${modelSelection.model} (${modelSelection.reason})`);
            this.client.setModel(modelSelection.model);
        }
        
        // ğŸ†• ì§ˆë¬¸ ìœ í˜•ì— ë§ê²Œ ì˜µì…˜ ì¡°ì •
        let chatOptions = adjustOptionsForModel(
            modelSelection.model,
            { ...modelSelection.options, ...(promptConfig.options || {}) },
            modelSelection.queryType
        );

        if (docId) {
            const docPreset = getGptOssTaskPreset('document');
            chatOptions = { ...docPreset, ...chatOptions };
        }

        const currentImages = [...(images || []), ...documentImages];

        // ğŸ†• ëª¨ë¸ í˜¸í™˜ì„± ì²´í¬ (ë„êµ¬ í˜¸ì¶œ ì§€ì› ì—¬ë¶€)
        const supportsTools = checkModelCapability(modelSelection.model, 'toolCalling');
        const supportsThinking = checkModelCapability(modelSelection.model, 'thinking');
        console.log(`[ChatService] ğŸ“Š ëª¨ë¸ ê¸°ëŠ¥: tools=${supportsTools}, thinking=${supportsThinking}`);



        // ğŸ—£ï¸ 6. LLM í˜¸ì¶œ (Chat vs Generate) with Agent Loop
        let metrics: Record<string, unknown> = {};
        // Â§9 Pipeline Profile: agentLoopMax ì ìš© (ê¸°ë³¸ 5)
        const maxTurns = executionPlan?.agentLoopMax ?? 5;
        let currentTurn = 0;
        let finalResponse = '';

        // Prepare initial history
        let currentHistory: ChatMessage[] = [];
        if (history && history.length > 0) {
            const combinedSystemPrompt = agentSystemMessage
                ? `${agentSystemMessage}\n\n---\n\n${promptConfig.systemPrompt}`
                : promptConfig.systemPrompt;

            currentHistory = [
                { role: 'system', content: combinedSystemPrompt },
                ...history.map((h) => ({
                    role: h.role as ChatMessage['role'],
                    content: h.content,
                    images: h.images
                }))
            ];
        } else {
            const combinedSystemPrompt = agentSystemMessage
                ? `${agentSystemMessage}\n\n---\n\n${promptConfig.systemPrompt}`
                : promptConfig.systemPrompt;
            currentHistory = [{ role: 'system', content: combinedSystemPrompt }];
        }

        // Add user message
        currentHistory.push({
            role: 'user',
            content: finalEnhancedMessage,
            ...(currentImages.length > 0 && { images: currentImages })
        });

        // Â§9 Pipeline Profile: A2A ì „ëµ ê²°ì •
        // - 'always': í•­ìƒ A2A ì‹¤í–‰
        // - 'conditional': ê¸°ë³¸ A2A ì‹œë„ (ê¸°ì¡´ ë™ì‘)
        // - 'off': A2A ê±´ë„ˆë›°ê³  ë‹¨ì¼ ëª¨ë¸ë§Œ ì‚¬ìš©
        const a2aStrategy = executionPlan?.profile?.a2a ?? 'conditional';
        const skipA2A = a2aStrategy === 'off';

        // ğŸ”€ A2A Multi-Model Parallel Answering (ê¸°ë³¸ í”Œë¡œìš°)
        let a2aSucceeded = false;
        if (!skipA2A) {
            try {
                checkAborted();
                console.log(`[ChatService] ğŸ”€ A2A ë³‘ë ¬ ì‘ë‹µ ì‹œì‘... (strategy: ${a2aStrategy})`);
                const a2aResponse = await this.processA2AParallel(
                    currentHistory,
                    chatOptions,
                    (token) => {
                        fullResponse += token;
                        onToken(token);
                    },
                    abortSignal
                );
                if (a2aResponse !== null) {
                    finalResponse = a2aResponse;
                    a2aSucceeded = true;
                    console.log('[ChatService] âœ… A2A ë³‘ë ¬ ì‘ë‹µ ì™„ë£Œ');
                }
            } catch (e) {
                if (e instanceof Error && e.message === 'ABORTED') throw e;
                console.warn('[ChatService] âš ï¸ A2A ì‹¤íŒ¨, ë‹¨ì¼ ëª¨ë¸ë¡œ í´ë°±:', e instanceof Error ? e.message : e);
            }
        } else {
            console.log('[ChatService] â­ï¸ A2A ê±´ë„ˆëœ€ (strategy: off)');
        }

        // ğŸ”„ A2A ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë‹¨ì¼ ëª¨ë¸ Agent Loop í´ë°±
        if (!a2aSucceeded) {
            console.log('[ChatService] ğŸ”„ ë‹¨ì¼ ëª¨ë¸ Agent Loop í´ë°±');

            // Agent Loop
            while (currentTurn < maxTurns) {
                // ğŸ†• ì¤‘ë‹¨ ì²´í¬
                checkAborted();

                currentTurn++;
                console.log(`[ChatService] ğŸ”„ Agent Loop Turn ${currentTurn}/${maxTurns}`);

                // ğŸ†• ëª¨ë¸ì´ ë„êµ¬ë¥¼ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ ë„êµ¬ ì „ë‹¬
                let allowedTools: ToolDefinition[] = [];
                if (supportsTools) {
                    // Prepare tools via ToolRouter (ë‚´ì¥+ì™¸ë¶€ ë„êµ¬ í†µí•©, ë“±ê¸‰ë³„ í•„í„°ë§)
                    const toolRouter = getUnifiedMCPClient().getToolRouter();
                    const userTierForTools = this.currentUserContext?.tier || 'free';
                    allowedTools = toolRouter.getOllamaTools(userTierForTools) as ToolDefinition[];
                }

                // Â§9 Pipeline Profile: thinking level ì ìš© (í”„ë¡œíŒŒì¼ > ì‚¬ìš©ì ìš”ì²­)
                const profileThinking = executionPlan?.thinkingLevel;
                const effectiveThinking = profileThinking && profileThinking !== 'off'
                    ? profileThinking
                    : (thinkingMode ? (thinkingLevel || 'high') : undefined);
                const thinkOption = (effectiveThinking && supportsThinking) ? effectiveThinking : undefined;
                
                const response = await this.client.chat(
                    currentHistory,
                    chatOptions,
                    (token) => {
                        // Only stream content tokens for the final answer or intermediate thoughts if we want
                        // For now, simple streaming of content
                        if (!token.includes('tool_calls')) {
                            fullResponse += token;
                            onToken(token);
                        }
                    },
                    {
                        tools: allowedTools.length > 0 ? allowedTools : undefined,
                        think: thinkOption  // ğŸ§  Ollama Native Thinking
                    }
                );

                // Capture metrics (accumulate or last?)
                // Ideally accumulate, but for now take the last one or significant one
                if (response.metrics) metrics = response.metrics as unknown as Record<string, unknown>;

                // Add assistant response to history
                const assistantMessage: ChatMessage = {
                    role: 'assistant',
                    content: response.content || '',
                    tool_calls: response.tool_calls
                };
                currentHistory.push(assistantMessage);

                // Check for tool calls
                if (response.tool_calls && response.tool_calls.length > 0) {
                    console.log(`[ChatService] ğŸ› ï¸ Tool Calls detected: ${response.tool_calls.length}`);

                    // Execute tools
                    for (const toolCall of response.tool_calls) {
                        const toolResult = await this.executeToolCall(toolCall);

                        // Add tool result to history
                        currentHistory.push({
                            role: 'tool',
                            content: toolResult, // Result must be string
                            // Ollama/OpenAI expects 'tool_call_id' reference usually, 
                            // but Ollama's current implementation might just need role: tool?
                            // Checking Ollama docs: messages should have 'role': 'tool', 'content': result
                            // And usually needs to match the function call.
                            // However, Ollama generic implementation details specifically for 'tool' role:
                            // "messages": [ ... { "role": "tool", "content": "..." } ]
                        });
                    }
                    // Loop continues to let LLM process the tool result
                } else {
                    // No tool calls, we are done
                    finalResponse = response.content || '';
                    break;
                }
            }
        }

        // ğŸ†• API ì‚¬ìš©ëŸ‰ ì¶”ì  (í‚¤ë³„ ì¶”ì  í¬í•¨) ë° ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
        try {
            const usageTracker = getApiUsageTracker();
            const keyManager = getApiKeyManager();
            const currentKey = keyManager.getCurrentKey();

            const responseTime = Date.now() - startTime;
            const tokenCount = fullResponse.length; // Fallback estimate

            // 1. API Usage Tracker (Persistent)
            usageTracker.recordRequest({
                tokens: tokenCount,
                responseTime: responseTime,
                model: this.client.model,
                apiKeyId: currentKey ? currentKey.substring(0, 8) : undefined,
                // Â§9 Pipeline Profile ID ì¶”ì 
                profileId: executionPlan?.isBrandModel ? executionPlan.requestedModel : undefined,
            });

            // 2. Metrics Collector (Real-time Memory)
            // ì§€ì—° ë¡œë”©ìœ¼ë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€ ê°€ëŠ¥ì„± ê³ ë ¤
            try {
                const { getMetrics } = require('../monitoring/metrics');
                const metricsCollector = getMetrics();

                metricsCollector.incrementCounter('chat_requests_total', 1, { model: this.client.model });
                metricsCollector.recordResponseTime(responseTime, this.client.model);
                metricsCollector.recordTokenUsage(tokenCount, this.client.model);

                if (currentKey) {
                    metricsCollector.incrementCounter('api_key_usage', 1, { keyId: currentKey.substring(0, 8) });
                }
            } catch (e) {
                console.warn('[ChatService] MetricsCollector ê¸°ë¡ ì‹¤íŒ¨:', e);
            }

            // 3. Analytics System (Analysis)
            try {
                const { getAnalyticsSystem } = require('../monitoring/analytics');
                const analytics = getAnalyticsSystem();

                // ì—ì´ì „íŠ¸ ì´ë¦„ í™•ì¸
                const agentName = selectedAgent ? selectedAgent.name : 'General Chat';
                const agentId = agentSelection?.primaryAgent || 'general';

                analytics.recordAgentRequest(
                    agentId,
                    agentName,
                    responseTime,
                    true, // success
                    tokenCount
                );

                // ì¿¼ë¦¬ ë¶„ì„ ê¸°ë¡
                analytics.recordQuery(message);
            } catch (e) {
                console.warn('[ChatService] AnalyticsSystem ê¸°ë¡ ì‹¤íŒ¨:', e);
            }

        } catch (e) {
            console.error('[ChatService] ëª¨ë‹ˆí„°ë§ ë°ì´í„° ê¸°ë¡ ì‹¤íŒ¨:', e);
        }

        return fullResponse;
    }

    /**
     * ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ëª¨ë“œë¡œ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
     * 
     * ì—¬ëŸ¬ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ê°€ í•˜ë‚˜ì˜ ì£¼ì œì— ëŒ€í•´ í† ë¡ í•˜ê³ ,
     * ê°ìì˜ ì˜ê²¬ì„ ì œì‹œí•œ í›„ ì¢…í•© ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
     * 
     * ğŸ†• ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì ìš©:
     * - ì—…ë¡œë“œëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
     * - ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
     * - ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì „ë‹¬
     * - ì‚¬ìš©ì ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬ (í–¥í›„ MemoryService ì—°ë™)
     * 
     * @param req - ì±„íŒ… ë©”ì‹œì§€ ìš”ì²­
     * @param uploadedDocuments - ì—…ë¡œë“œëœ ë¬¸ì„œ ë§µ
     * @param onToken - í† í° ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
     * @param onProgress - í† ë¡  ì§„í–‰ ìƒí™© ì½œë°± (ì„ íƒì )
     * @returns í¬ë§·íŒ…ëœ í† ë¡  ê²°ê³¼ ë¬¸ìì—´
     */
    async processMessageWithDiscussion(
        req: ChatMessageRequest,
        uploadedDocuments: DocumentStore,
        onToken: (token: string) => void,
        onProgress?: (progress: DiscussionProgress) => void
    ): Promise<string> {
        const { message, docId, history, webSearchContext, images, userId } = req;

        console.log('[ChatService] ğŸ¯ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ëª¨ë“œ ì‹œì‘');

        // ========================================
        // ğŸ†• ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§: ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        // ========================================
        
        // 1. ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        let documentContext = '';
        let documentImages: string[] = [];
        
        if (docId) {
            const doc = uploadedDocuments.get(docId);
            if (doc) {
                let docText = doc.text || '';
                const maxChars = 30000; // í† ë¡  ëª¨ë“œì—ì„œëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
                
                if (docText.length > maxChars) {
                    const half = Math.floor(maxChars / 2);
                    docText = `${docText.substring(0, half)}\n... [ì¤‘ê°„ ìƒëµ] ...\n${docText.substring(docText.length - half)}`;
                }
                
                documentContext = `ğŸ“š ë¬¸ì„œ: ${doc.filename} (${doc.type})\n` +
                    `ê¸¸ì´: ${doc.text.length}ì\n\n${docText}`;
                    
                console.log(`[ChatService] ğŸ“„ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì ìš©: ${doc.filename} (${docText.length}ì)`);
                
                // ğŸ†• ì´ë¯¸ì§€/PDFì—ì„œ ë¹„ì „ ë°ì´í„° ì¶”ì¶œ
                if (['image', 'pdf'].includes(doc.type) && doc.info?.base64) {
                    documentImages.push(doc.info.base64);
                    console.log(`[ChatService] ğŸ–¼ï¸ ë¬¸ì„œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œë¨`);
                }
            }
        }
        
        // 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ ë³€í™˜
        const conversationHistory = history?.map(h => ({
            role: h.role as string,
            content: h.content as string
        })) || [];
        
        if (conversationHistory.length > 0) {
            console.log(`[ChatService] ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì ìš©: ${conversationHistory.length}ê°œ ë©”ì‹œì§€`);
        }
        
        // 3. ì›¹ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
        if (webSearchContext) {
            console.log(`[ChatService] ğŸ” ì›¹ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì ìš©: ${webSearchContext.length}ì`);
        }
        
        // ğŸ†• 4. ì‚¬ìš©ì ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ (MemoryService ì—°ë™)
        let userMemoryContext = '';
        if (userId && userId !== 'guest') {
            try {
                const { getMemoryService } = await import('./MemoryService');
                const memoryService = getMemoryService();
                const memoryResult = await memoryService.buildMemoryContext(userId, message);
                
                if (memoryResult.contextString) {
                    userMemoryContext = memoryResult.contextString;
                    console.log(`[ChatService] ğŸ’¾ ì‚¬ìš©ì ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì ìš©: ${memoryResult.memories.length}ê°œ ê¸°ì–µ, ${userMemoryContext.length}ì`);
                }
            } catch (e) {
                console.warn('[ChatService] MemoryService ë¡œë“œ ì‹¤íŒ¨:', e);
            }
        }
        
        // ğŸ†• 5. ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ìš”ì²­ì—ì„œ ì˜¨ ì´ë¯¸ì§€ + ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€)
        const allImages = [...(images || []), ...documentImages];
        let imageDescriptions: string[] = [];
        
        // ğŸ†• ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë¹„ì „ ëª¨ë¸ë¡œ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ ì„¤ëª… ìƒì„±
        if (allImages.length > 0) {
            console.log(`[ChatService] ğŸ–¼ï¸ ${allImages.length}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘...`);
            
            onProgress?.({
                phase: 'selecting',
                message: `${allImages.length}ê°œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...`,
                progress: 2
            });
            
            const imagePromises = allImages.slice(0, 3).map(async (imageBase64, i) => {
                try {
                    const analysisResponse = await this.client.chat(
                        [
                            { 
                                role: 'system', 
                                content: 'ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ 200ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸, ë„í‘œ, ê·¸ë˜í”„ê°€ ìˆë‹¤ë©´ í•´ë‹¹ ë‚´ìš©ë„ í¬í•¨í•˜ì„¸ìš”.' 
                            },
                            {
                                role: 'user',
                                content: 'ì´ ì´ë¯¸ì§€ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.',
                                images: [imageBase64]
                            }
                        ],
                        { temperature: 0.2 }
                    );
                    
                    if (analysisResponse.content) {
                        console.log(`[ChatService] âœ… ì´ë¯¸ì§€ ${i + 1} ë¶„ì„ ì™„ë£Œ`);
                        return analysisResponse.content.substring(0, 500);
                    }
                    return `[ì´ë¯¸ì§€ ${i + 1}: ë‚´ìš© ì—†ìŒ]`;
                } catch (e) {
                    console.warn(`[ChatService] ì´ë¯¸ì§€ ${i + 1} ë¶„ì„ ì‹¤íŒ¨:`, e);
                    return `[ì´ë¯¸ì§€ ${i + 1}: ë¶„ì„ ì‹¤íŒ¨]`;
                }
            });
            imageDescriptions = await Promise.all(imagePromises);
        }

        // LLM í˜¸ì¶œ ë˜í¼ í•¨ìˆ˜
        const generateResponse = async (systemPrompt: string, userMessage: string): Promise<string> => {
            let response = '';
            const chatMessages: ChatMessage[] = [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: userMessage }
            ];

            await this.client.chat(chatMessages, {}, (token) => {
                response += token;
            });

            return response;
        };

        // ğŸ†• í† ë¡  ì—”ì§„ ìƒì„± (ì™„ì „í•œ ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì ìš©)
        const discussionEngine = createDiscussionEngine(
            generateResponse,
            { 
                maxAgents: 5, 
                enableCrossReview: true,
                enableDeepThinking: true,
                
                // ğŸ†• ì»¨í…ìŠ¤íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í•„ë“œ ì „ë‹¬
                documentContext,
                conversationHistory,
                userMemoryContext,
                webSearchContext,
                
                // ğŸ†• ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ (ë¶„ì„ ê²°ê³¼ + ì›ë³¸ ë°ì´í„°)
                imageContexts: allImages,
                imageDescriptions,
                
                // ğŸ†• ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ ì„¤ì • (ì‚¬ìš©ì ë©”ëª¨ë¦¬ ìµœìš°ì„ )
                contextPriority: {
                    userMemory: 1,
                    conversationHistory: 2,
                    document: 3,
                    webSearch: 4,
                    image: 5
                },
                
                // ğŸ†• í† í° ì œí•œ ì„¤ì •
                tokenLimits: {
                    maxTotalTokens: 10000,  // í† ë¡  ëª¨ë“œëŠ” ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í—ˆìš©
                    maxDocumentTokens: 4000,
                    maxHistoryTokens: 2000,
                    maxWebSearchTokens: 2000,
                    maxMemoryTokens: 1500,
                    maxImageDescriptionTokens: 500
                }
            },
            onProgress
        );

        // ì›¹ ê²€ìƒ‰ í•¨ìˆ˜ ë¡œë“œ (ì‚¬ì‹¤ ê²€ì¦ìš©)
        let webSearchFn: ((q: string, opts?: { maxResults?: number }) => Promise<WebSearchResult[]>) | undefined;
        try {
            const { performWebSearch } = await import('../mcp');
            webSearchFn = performWebSearch;
            console.log('[ChatService] ğŸ” ì›¹ ê²€ìƒ‰ ì‚¬ì‹¤ ê²€ì¦ í™œì„±í™”');
        } catch (e) {
            console.warn('[ChatService] ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨, ì‚¬ì‹¤ ê²€ì¦ ë¹„í™œì„±í™”');
        }

        // í† ë¡  ì‹¤í–‰ (ì›¹ ê²€ìƒ‰ ì‚¬ì‹¤ ê²€ì¦ í¬í•¨)
        const result: DiscussionResult = await discussionEngine.startDiscussion(message, webSearchFn);

        // í† ë¡  ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì†¡
        const formattedResponse = this.formatDiscussionResult(result);

        // í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
        for (const char of formattedResponse) {
            onToken(char);
        }

        // ğŸ†• ìƒì„¸ ë¡œê·¸
        console.log(`[ChatService] ğŸ¯ í† ë¡  ì™„ë£Œ: ${result.totalTime}ms, ì°¸ì—¬ì: ${result.participants.length}ëª…`);
        console.log(`[ChatService] ğŸ“Š ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© í˜„í™©:`);
        console.log(`   - ë¬¸ì„œ: ${documentContext ? 'âœ“' : 'âœ—'} (${documentContext.length}ì)`);
        console.log(`   - íˆìŠ¤í† ë¦¬: ${conversationHistory.length}ê°œ ë©”ì‹œì§€`);
        console.log(`   - ë©”ëª¨ë¦¬: ${userMemoryContext ? 'âœ“' : 'âœ—'} (${userMemoryContext.length}ì)`);
        console.log(`   - ì›¹ê²€ìƒ‰: ${webSearchContext ? 'âœ“' : 'âœ—'}`);
        console.log(`   - ì´ë¯¸ì§€: ${imageDescriptions.length}ê°œ ë¶„ì„ë¨`);

        return formattedResponse;
    }

    /**
     * ğŸ”¬ Deep Research ëª¨ë“œ ë©”ì‹œì§€ ì²˜ë¦¬
     * 
     * ì‹¬ì¸µ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ ì£¼ì œì— ëŒ€í•œ ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
     * - ì£¼ì œ ë¶„í•´ â†’ ì›¹ ê²€ìƒ‰ â†’ LLM í•©ì„± â†’ ë°˜ë³µ ë£¨í”„ â†’ ë³´ê³ ì„œ ìƒì„±
     * 
     * @param req - ì±„íŒ… ë©”ì‹œì§€ ìš”ì²­
     * @param onToken - í† í° ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
     * @param onProgress - ì—°êµ¬ ì§„í–‰ ìƒí™© ì½œë°± (ì„ íƒì )
     * @returns ì—°êµ¬ ë³´ê³ ì„œ ë¬¸ìì—´
     */
    async processMessageWithDeepResearch(
        req: ChatMessageRequest,
        onToken: (token: string) => void,
        onProgress?: (progress: ResearchProgress) => void
    ): Promise<string> {
        const { message, userId } = req;

        console.log('[ChatService] ğŸ”¬ Deep Research ëª¨ë“œ ì‹œì‘');

        // DeepResearchService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (50-100ê°œ ì†ŒìŠ¤ ì‹¬ì¸µ ì¡°ì‚¬)
        const researchService = new DeepResearchService({
            maxLoops: 5,
            llmModel: this.client.model,
            searchApi: 'all',
            maxSearchResults: 360,
            language: 'ko',
            maxTotalSources: 80,
            scrapeFullContent: true,
            maxScrapePerLoop: 15,
            scrapeTimeoutMs: 15000,
            chunkSize: 10
        });

        // ì„¸ì…˜ ID ìƒì„±
        const sessionId = uuidv4();

        // DBì— ë¦¬ì„œì¹˜ ì„¸ì…˜ ìƒì„± (executeResearch ì „ì— í•„ìˆ˜ â€” FK ì œì•½ì¡°ê±´)
        const db = getUnifiedDatabase();
        await db.createResearchSession({
            id: sessionId,
            userId: userId && userId !== 'guest' && !userId.startsWith('anon-') ? userId : undefined,
            topic: message,
            depth: 'deep'
        });

        // ì—°êµ¬ ì‹œì‘
        const result = await researchService.executeResearch(
            sessionId,
            message,
            onProgress
        );

        // ì—°êµ¬ ê²°ê³¼ë¥¼ í¬ë§·íŒ…
        const formattedResponse = this.formatResearchResult(result);

        // í† í° ìŠ¤íŠ¸ë¦¬ë°
        for (const char of formattedResponse) {
            onToken(char);
        }

        console.log(`[ChatService] ğŸ”¬ Deep Research ì™„ë£Œ: ${result.duration}ms, ${result.totalSteps} ë‹¨ê³„`);

        return formattedResponse;
    }

    /**
     * ğŸ”¬ Deep Research ê²°ê³¼ í¬ë§·íŒ…
     */
    private formatResearchResult(result: { topic: string; summary: string; keyFindings: string[]; sources: Array<{ title: string; url: string }>; totalSteps: number; duration: number }): string {
        const sections = [
            `# ğŸ”¬ ì‹¬ì¸µ ì—°êµ¬ ë³´ê³ ì„œ: ${result.topic}`,
            '',
            '## ğŸ“‹ ì¢…í•© ìš”ì•½',
            result.summary,
            '',
            '## ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­',
            ...result.keyFindings.map((finding, i) => `${i + 1}. ${finding}`),
            '',
            '## ğŸ“š ì°¸ê³  ìë£Œ',
            ...result.sources.map((source, i) => `[${i + 1}] [${source.title}](${source.url})`),
            '',
            `---`,
            `*ì´ ${result.totalSteps}ë‹¨ê³„ ì—°êµ¬, ${result.sources.length}ê°œ ì†ŒìŠ¤ ë¶„ì„, ${(result.duration / 1000).toFixed(1)}ì´ˆ ì†Œìš”*`
        ];

        return sections.join('\n');
    }

    // ============================================================
    // ğŸ”€ A2A Multi-Model Parallel Answering
    // ============================================================

    /**
     * A2A ë³‘ë ¬ ì‘ë‹µ: ë‘ ëª¨ë¸ì´ ë™ì‹œì— ë‹µë³€ â†’ ì¢…í•© ëª¨ë¸ì´ í•©ì„±
     * 
     * @param messages - ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ (system + history + user)
     * @param chatOptions - ëª¨ë¸ ì˜µì…˜
     * @param onToken - ì¢…í•© ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
     * @param abortSignal - ì¤‘ë‹¨ ì‹œê·¸ë„
     * @returns ì¢…í•©ëœ ìµœì¢… ì‘ë‹µ ë¬¸ìì—´, ë˜ëŠ” null (ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨ ì‹œ â†’ í´ë°± íŠ¸ë¦¬ê±°)
     */
    private async processA2AParallel(
        messages: ChatMessage[],
        chatOptions: ModelOptions,
        onToken: (token: string) => void,
        abortSignal?: AbortSignal
    ): Promise<string | null> {
        const startTime = Date.now();

        // 1. ë‘ ëª¨ë¸ìš© OllamaClient ìƒì„±
        const clientA = new OllamaClient({ model: A2A_MODELS.primary });
        const clientB = new OllamaClient({ model: A2A_MODELS.secondary });

        console.log(`[ChatService] ğŸ”€ A2A ë³‘ë ¬ ìš”ì²­: ${A2A_MODELS.primary} + ${A2A_MODELS.secondary}`);

        // 2. ë³‘ë ¬ ìš”ì²­ (ìŠ¤íŠ¸ë¦¬ë° ì—†ì´, ì „ì²´ ì‘ë‹µ ìˆ˜ì§‘)
        const [resultA, resultB] = await Promise.allSettled([
            clientA.chat(messages, chatOptions),
            clientB.chat(messages, chatOptions),
        ]);

        // ì¤‘ë‹¨ ì²´í¬
        if (abortSignal?.aborted) {
            throw new Error('ABORTED');
        }

        const responseA = resultA.status === 'fulfilled' ? resultA.value.content : null;
        const responseB = resultB.status === 'fulfilled' ? resultB.value.content : null;
        const durationParallel = Date.now() - startTime;

        console.log(`[ChatService] ğŸ”€ A2A ë³‘ë ¬ ì™„ë£Œ (${durationParallel}ms): ` +
            `${A2A_MODELS.primary}=${resultA.status}, ${A2A_MODELS.secondary}=${resultB.status}`);

        // 3. ê²°ê³¼ ì²˜ë¦¬
        if (!responseA && !responseB) {
            // ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨ â†’ null ë°˜í™˜ (í´ë°± íŠ¸ë¦¬ê±°)
            console.warn('[ChatService] âš ï¸ A2A ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨');
            if (resultA.status === 'rejected') console.warn(`  ${A2A_MODELS.primary}: ${resultA.reason}`);
            if (resultB.status === 'rejected') console.warn(`  ${A2A_MODELS.secondary}: ${resultB.reason}`);
            return null;
        }

        if (!responseA || !responseB) {
            // í•œìª½ë§Œ ì„±ê³µ â†’ ì„±ê³µí•œ ì‘ë‹µ ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë°
            const singleResponse = (responseA || responseB) as string;
            const succeededModel = responseA ? A2A_MODELS.primary : A2A_MODELS.secondary;
            console.log(`[ChatService] ğŸ”€ A2A ë‹¨ì¼ ì‘ë‹µ ì‚¬ìš©: ${succeededModel}`);

            // A2A í‘œì‹œ í—¤ë” ì¶”ê°€
            const header = `> ğŸ¤– *${succeededModel} ë‹¨ë… ì‘ë‹µ*\n\n`;
            for (const char of header) { onToken(char); }
            for (const char of singleResponse) { onToken(char); }
            return header + singleResponse;
        }

        // 4. ì–‘ìª½ ëª¨ë‘ ì„±ê³µ â†’ ì¢…í•© í•©ì„±
        console.log(`[ChatService] ğŸ”€ A2A ì¢…í•© í•©ì„± ì‹œì‘ (synthesizer: ${A2A_MODELS.synthesizer})`);

        // ì›ë³¸ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ (ë§ˆì§€ë§‰ user ë©”ì‹œì§€)
        const userMessage = [...messages].reverse().find(m => m.role === 'user')?.content || '';

        const synthesisUserMessage = [
            `## ì›ë³¸ ì§ˆë¬¸`,
            userMessage,
            '',
            `## Response A (${A2A_MODELS.primary})`,
            responseA,
            '',
            `## Response B (${A2A_MODELS.secondary})`,
            responseB,
            '',
            'ìœ„ ë‘ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ìµœê³  í’ˆì§ˆì˜ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.',
        ].join('\n');

        const synthesizerClient = new OllamaClient({ model: A2A_MODELS.synthesizer });
        let fullSynthesis = '';

        // A2A í‘œì‹œ í—¤ë” ìŠ¤íŠ¸ë¦¬ë°
        const header = `> ğŸ”€ *${A2A_MODELS.primary} + ${A2A_MODELS.secondary} A2A ì¢…í•© ë‹µë³€*\n\n`;
        for (const char of header) { onToken(char); }

        // ì¢…í•© ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        await synthesizerClient.chat(
            [
                { role: 'system', content: A2A_SYNTHESIS_SYSTEM_PROMPT },
                { role: 'user', content: synthesisUserMessage },
            ],
            { temperature: 0.3 },
            (token) => {
                fullSynthesis += token;
                onToken(token);
            }
        );

        const totalDuration = Date.now() - startTime;
        console.log(`[ChatService] âœ… A2A ì¢…í•© ì™„ë£Œ: ë³‘ë ¬=${durationParallel}ms, í•©ì„±=${totalDuration - durationParallel}ms, ì´=${totalDuration}ms`);

        return header + fullSynthesis;
    }

    /**
     * ë‹¨ì¼ MCP ë„êµ¬ í˜¸ì¶œì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
     * 
     * ğŸ†• ë“±ê¸‰ë³„ ê¶Œí•œ ê²€ì¦ ì ìš©:
     * - admin â†’ enterprise (ëª¨ë“  ë„êµ¬ í—ˆìš©)
     * - user â†’ ë“±ê¸‰ì— ë”°ë¼ ì œí•œ
     * - guest â†’ free (ê¸°ë³¸ ë„êµ¬ë§Œ)
     * 
     * ì§€ì› ë„êµ¬:
     * - web_search: ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
     * - web_fetch: ì›¹í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ
     * - vision_ocr: ì´ë¯¸ì§€ OCR
     * - analyze_image: ì´ë¯¸ì§€ ë¶„ì„
     * - ê¸°íƒ€ MCP ê¸°ë³¸ ë„êµ¬
     * 
     * @param toolCall - ë„êµ¬ í˜¸ì¶œ ì •ë³´ (function.name, function.arguments)
     * @returns ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ë¬¸ìì—´
     * @private
     */
    private async executeToolCall(toolCall: ToolCallInfo): Promise<string> {
        if (!toolCall.function || !toolCall.function.name) return 'Error: Invalid tool call';

        const toolName = toolCall.function.name;
        const toolArgs = toolCall.function.arguments;

        // ğŸ†• ë“±ê¸‰ë³„ ë„êµ¬ ì ‘ê·¼ ê¶Œí•œ ê²€ì¦
        if (this.currentUserContext) {
            const userTier = this.currentUserContext.tier;
            if (!canUseTool(userTier, toolName)) {
                const tierLabel = {
                    'free': 'ë¬´ë£Œ',
                    'pro': 'í”„ë¡œ',
                    'enterprise': 'ì—”í„°í”„ë¼ì´ì¦ˆ'
                }[userTier];
                
                console.warn(`[ChatService] âš ï¸ ë„êµ¬ ì ‘ê·¼ ê±°ë¶€: ${toolName} (tier: ${userTier})`);
                return `ğŸ”’ ê¶Œí•œ ì—†ìŒ: ${tierLabel} ë“±ê¸‰ì—ì„œëŠ” "${toolName}" ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—…ê·¸ë ˆì´ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.`;
            }
        }

        console.log(`[ChatService] ğŸ”¨ Executing Tool: ${toolName} (tier: ${this.currentUserContext?.tier || 'unknown'})`, toolArgs);

        // ğŸŒ Ollama ë„¤ì´í‹°ë¸Œ ì›¹ ê²€ìƒ‰/ì¶”ì¶œ ë„êµ¬ ì²˜ë¦¬
        if (toolName === 'web_search') {
            try {
                const query = toolArgs.query as string;
                const maxResults = (toolArgs.max_results as number) || 5;
                const response = await this.client.webSearch(query, maxResults);

                if (response.results && response.results.length > 0) {
                    const formatted = response.results.map((r, i) =>
                        `[${i + 1}] ${r.title}\n    ${r.url}\n    ${r.content?.substring(0, 200) || ''}...`
                    ).join('\n\n');
                    return `ğŸ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ (${response.results.length}ê°œ):\n\n${formatted}`;
                }
                return 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.';
            } catch (e: unknown) {
                const errorMessage = e instanceof Error ? e.message : String(e);
                console.error('[ChatService] web_search ì‹¤í–‰ ì‹¤íŒ¨:', errorMessage);
                return `Error: ${errorMessage}`;
            }
        }

        if (toolName === 'web_fetch') {
            try {
                const url = toolArgs.url as string;
                const response = await this.client.webFetch(url);

                if (response.content) {
                    return `ğŸ“¥ ì›¹í˜ì´ì§€: ${response.title}\n\n${response.content.substring(0, 3000)}`;
                }
                return 'í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
            } catch (e: unknown) {
                const errorMessage = e instanceof Error ? e.message : String(e);
                console.error('[ChatService] web_fetch ì‹¤í–‰ ì‹¤íŒ¨:', errorMessage);
                return `Error: ${errorMessage}`;
            }
        }

        // ğŸ–¼ï¸ Vision OCR - ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if (toolName === 'vision_ocr') {
            try {
                const imagePath = toolArgs.image_path as string;
                const imageBase64 = toolArgs.image_base64 as string;
                const language = (toolArgs.language as string) || 'auto';

                let imageData: string;
                if (imageBase64) {
                    imageData = imageBase64;
                } else if (imagePath) {
                    // ğŸ”’ ë³´ì•ˆ íŒ¨ì¹˜ 2026-02-07: ê²½ë¡œ íƒìƒ‰ ë°©ì–´ â€” UserSandbox ê²€ì¦ ì ìš©
                    const { UserSandbox } = await import('../mcp/user-sandbox');
                    const userId = this.currentUserContext?.userId || 'guest';
                    const safePath = UserSandbox.resolvePath(userId, imagePath);
                    if (!safePath) {
                        return 'Error: ì ‘ê·¼ ê¶Œí•œì´ ì—†ëŠ” ê²½ë¡œì…ë‹ˆë‹¤. ì‚¬ìš©ì ì‘ì—… ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.';
                    }
                    const { readFile } = await import('fs/promises');
                    const fileBuffer = await readFile(safePath);
                    imageData = fileBuffer.toString('base64');
                } else {
                    return 'Error: image_path ë˜ëŠ” image_base64ê°€ í•„ìš”í•©ë‹ˆë‹¤.';
                }

                console.log(`[ChatService] ğŸ” Vision OCR ì‹¤í–‰ ì¤‘...`);

                // Gemini Visionì„ í†µí•œ OCR
                const ocrResponse = await this.client.chat(
                    [
                        { role: 'system', content: 'You are an OCR expert. Extract ALL text from the image exactly as it appears. Preserve formatting, line breaks, and structure. If the text is in Korean, Japanese, or Chinese, output it in the original language.' },
                        {
                            role: 'user',
                            content: `ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì›ë³¸ í˜•ì‹ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ì„¸ìš”.${language !== 'auto' ? ` ì–¸ì–´: ${language}` : ''}`,
                            images: [imageData]
                        }
                    ],
                    { temperature: 0.1 }  // ë‚®ì€ temperatureë¡œ ì •í™•ë„ í–¥ìƒ
                );

                const extractedText = ocrResponse.content || '';
                console.log(`[ChatService] âœ… OCR ì™„ë£Œ: ${extractedText.length}ì ì¶”ì¶œ`);

                return `ğŸ“ OCR ê²°ê³¼:\n\n${extractedText}`;
            } catch (e: unknown) {
                const errorMessage = e instanceof Error ? e.message : String(e);
                console.error('[ChatService] vision_ocr ì‹¤í–‰ ì‹¤íŒ¨:', errorMessage);
                return `Error: ${errorMessage}`;
            }
        }

        // ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ - ì´ë¯¸ì§€ ë‚´ìš© ë¶„ì„ ë° ì„¤ëª…
        if (toolName === 'analyze_image') {
            try {
                const imagePath = toolArgs.image_path as string;
                const imageBase64 = toolArgs.image_base64 as string;
                const question = (toolArgs.question as string) || 'ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”? ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.';

                let imageData: string;
                if (imageBase64) {
                    imageData = imageBase64;
                } else if (imagePath) {
                    // ğŸ”’ ë³´ì•ˆ íŒ¨ì¹˜ 2026-02-07: ê²½ë¡œ íƒìƒ‰ ë°©ì–´ â€” UserSandbox ê²€ì¦ ì ìš©
                    const { UserSandbox } = await import('../mcp/user-sandbox');
                    const userId = this.currentUserContext?.userId || 'guest';
                    const safePath = UserSandbox.resolvePath(userId, imagePath);
                    if (!safePath) {
                        return 'Error: ì ‘ê·¼ ê¶Œí•œì´ ì—†ëŠ” ê²½ë¡œì…ë‹ˆë‹¤. ì‚¬ìš©ì ì‘ì—… ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.';
                    }
                    const { readFile } = await import('fs/promises');
                    const fileBuffer = await readFile(safePath);
                    imageData = fileBuffer.toString('base64');
                } else {
                    return 'Error: image_path ë˜ëŠ” image_base64ê°€ í•„ìš”í•©ë‹ˆë‹¤.';
                }

                console.log(`[ChatService] ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤í–‰ ì¤‘...`);

                const analysisResponse = await this.client.chat(
                    [
                        { role: 'system', content: 'You are an expert image analyst. Describe images in detail, including objects, text, colors, composition, and any relevant context.' },
                        {
                            role: 'user',
                            content: question,
                            images: [imageData]
                        }
                    ],
                    { temperature: 0.3 }
                );

                const analysis = analysisResponse.content || '';
                console.log(`[ChatService] âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ`);

                return `ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n\n${analysis}`;
            } catch (e: unknown) {
                const errorMessage = e instanceof Error ? e.message : String(e);
                console.error('[ChatService] analyze_image ì‹¤í–‰ ì‹¤íŒ¨:', errorMessage);
                return `Error: ${errorMessage}`;
            }
        }

        // âš™ï¸ Phase 3: ë„êµ¬ ì‹¤í–‰ ì‹œ UserContext ì „ë‹¬ (2026-02-07)
        // ë„êµ¬ ì‹¤í–‰ (ToolRouter ê²½ìœ  â€” ë‚´ì¥+ì™¸ë¶€ ë„êµ¬ í†µí•© ë¼ìš°íŒ…)
        try {
            const toolRouter = getUnifiedMCPClient().getToolRouter();
            const result = await toolRouter.executeTool(toolName, toolArgs, this.currentUserContext ?? undefined);
            if (result.isError) {
                return `Error executing tool: ${result.content.map((c: { text?: string }) => c.text).join('\n')}`;
            }
            return result.content.map((c: { text?: string }) => c.text).join('\n');
        } catch (e: unknown) {
            const errorMessage = e instanceof Error ? e.message : String(e);
            console.error(`[ChatService] Tool execution failed: ${errorMessage}`);
            return `Error: ${errorMessage}`;
        }
    }

    /**
     * í† ë¡  ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
     * 
     * í¬í•¨ ë‚´ìš©:
     * - í† ë¡  ìš”ì•½ í—¤ë”
     * - ì°¸ì—¬ ì „ë¬¸ê°€ë³„ ì˜ê²¬ (GPT ìŠ¤íƒ€ì¼ Thinking ë¸”ë¡ìœ¼ë¡œ ê¸°ë³¸ í¼ì¹¨)
     * - ìµœì¢… ì¢…í•© ë‹µë³€ (ì ‘í˜ ê°€ëŠ¥)
     * 
     * @param result - í† ë¡  ì—”ì§„ì˜ ê²°ê³¼ ê°ì²´
     * @returns ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…ëœ ë¬¸ìì—´
     * @private
     */
    private formatDiscussionResult(result: DiscussionResult): string {
        let formatted = '';

        // í† ë¡  ìš”ì•½ í—¤ë”
        formatted += `## ğŸ¯ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ê²°ê³¼\n\n`;
        formatted += `> ${result.discussionSummary}\n\n`;
        formatted += `---\n\n`;

        // ğŸ†• ì°¸ì—¬ ì „ë¬¸ê°€ ì˜ê²¬ (GPT ìŠ¤íƒ€ì¼ ê¸°ë³¸ í¼ì¹¨ ìƒíƒœ)
        formatted += `## ğŸ“‹ ì „ë¬¸ê°€ë³„ ë¶„ì„\n\n`;

        for (const opinion of result.opinions) {
            // GPT ìŠ¤íƒ€ì¼ Thinking ë¸”ë¡
            formatted += `### ${opinion.agentEmoji} ${opinion.agentName}\n\n`;
            formatted += `> ğŸ’­ **Thinking**: ${opinion.agentName} ê´€ì ì—ì„œ ë¶„ì„ ì¤‘...\n\n`;
            formatted += `${opinion.opinion}\n\n`;
            formatted += `---\n\n`;
        }

        // ğŸ†• ìµœì¢… ì¢…í•© ë‹µë³€ (ì ‘í˜ ê°€ëŠ¥ - ì„ íƒì  í™•ì¸)
        formatted += `<details open>\n<summary>ğŸ’¡ <strong>ì¢…í•© ë‹µë³€</strong> (ì „ë¬¸ê°€ ì˜ê²¬ ì¢…í•©)</summary>\n\n`;
        formatted += result.finalAnswer;
        formatted += `\n\n</details>`;

        return formatted;
    }
}
