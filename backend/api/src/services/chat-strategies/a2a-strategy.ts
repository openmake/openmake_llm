/**
 * ============================================================
 * A2AStrategy - Agent-to-Agent ë³‘ë ¬ ìƒì„± ì „ëµ
 * ============================================================
 *
 * ë‘ ê°œì˜ LLM ëª¨ë¸ì— ë™ì‹œì— ìš”ì²­í•˜ì—¬ ë³‘ë ¬ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ê³ ,
 * í•©ì„± ëª¨ë¸ì´ ë‘ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ìµœê³  í’ˆì§ˆì˜ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
 *
 * @module services/chat-strategies/a2a-strategy
 * @description
 * - Primary + Secondary ëª¨ë¸ ë³‘ë ¬ í˜¸ì¶œ (Promise.allSettled)
 * - ì–‘ìª½ ëª¨ë‘ ì„±ê³µ ì‹œ Synthesizer ëª¨ë¸ì´ ì‘ë‹µ ì¢…í•©
 * - í•œìª½ë§Œ ì„±ê³µ ì‹œ í•´ë‹¹ ì‘ë‹µì„ ë‹¨ë… ì‚¬ìš©
 * - ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨ ì‹œ succeeded=false ë°˜í™˜ (AgentLoop í´ë°± íŠ¸ë¦¬ê±°)
 */
import { OllamaClient } from '../../ollama/client';
import type { ChatStrategy, A2AStrategyContext, A2AStrategyResult } from './types';
import { createLogger } from '../../utils/logger';

const logger = createLogger('A2AStrategy');

/**
 * A2A ëª¨ë¸ ì¡°í•© íƒ€ì…
 * resolveA2AModels()ê°€ ë°˜í™˜í•˜ëŠ” primary/secondary/synthesizer ëª¨ë¸ ì„¸íŠ¸
 */
interface A2AModelSelection {
    /** 1ì°¨ ì‘ë‹µ ìƒì„± ëª¨ë¸ */
    primary: string;
    /** 2ì°¨ ì‘ë‹µ ìƒì„± ëª¨ë¸ */
    secondary: string;
    /** ë‘ ì‘ë‹µì„ ì¢…í•©í•˜ëŠ” í•©ì„± ëª¨ë¸ */
    synthesizer: string;
}

/**
 * QueryType ë¯¸ì§€ì • ë˜ëŠ” ë§¤í•‘ ì—†ëŠ” ìœ í˜•ì— ëŒ€í•œ ê¸°ë³¸ A2A ëª¨ë¸ ì¡°í•©
 * @constant
 */
const DEFAULT_A2A_MODELS: A2AModelSelection = {
    primary: 'gpt-oss:120b-cloud',
    secondary: 'gemini-3-flash-preview:cloud',
    synthesizer: 'gemini-3-flash-preview:cloud',
};

/**
 * ì§ˆë¬¸ ìœ í˜•(QueryType)ì— ë”°ë¼ ìµœì ì˜ A2A ëª¨ë¸ ì¡°í•©ì„ ì„ íƒí•©ë‹ˆë‹¤.
 *
 * ollama listì—ì„œ í™•ì¸ëœ ëª¨ë¸ë§Œ ì‚¬ìš©:
 * - gemini-3-flash-preview:cloud (ë²”ìš© Fast)
 * - gpt-oss:120b-cloud (ëŒ€í˜• ë²”ìš©)
 * - qwen3-coder-next:cloud (ì½”ë“œ íŠ¹í™”)
 * - kimi-k2.5:cloud (ë²”ìš©)
 * - qwen3-vl:235b-cloud (ë¹„ì „ íŠ¹í™”)
 *
 * @param queryType - ì‚¬ìš©ì ì§ˆë¬¸ ìœ í˜• (code/math/creative/analysis/chat/vision ë“±)
 * @returns ìµœì ì˜ A2A ëª¨ë¸ ì¡°í•© (primary + secondary + synthesizer)
 */
function resolveA2AModels(queryType?: string): A2AModelSelection {
    switch (queryType) {
        case 'code':
            return {
                primary: 'qwen3-coder-next:cloud',
                secondary: 'gpt-oss:120b-cloud',
                synthesizer: 'gemini-3-flash-preview:cloud',
            };
        case 'math':
            return {
                primary: 'gpt-oss:120b-cloud',
                secondary: 'kimi-k2.5:cloud',
                synthesizer: 'gemini-3-flash-preview:cloud',
            };
        case 'creative':
            return {
                primary: 'gpt-oss:120b-cloud',
                secondary: 'kimi-k2.5:cloud',
                synthesizer: 'gemini-3-flash-preview:cloud',
            };
        case 'analysis':
            return {
                primary: 'gpt-oss:120b-cloud',
                secondary: 'qwen3-coder-next:cloud',
                synthesizer: 'gemini-3-flash-preview:cloud',
            };
        case 'chat':
            return {
                primary: 'gemini-3-flash-preview:cloud',
                secondary: 'gpt-oss:120b-cloud',
                synthesizer: 'gemini-3-flash-preview:cloud',
            };
        case 'vision':
            return {
                primary: 'qwen3-vl:235b-cloud',
                secondary: 'gpt-oss:120b-cloud',
                synthesizer: 'gemini-3-flash-preview:cloud',
            };
        default:
            return DEFAULT_A2A_MODELS;
    }
}

/** A2A í•©ì„± ëª¨ë¸ì— ì „ë‹¬ë˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ */
const A2A_SYNTHESIS_SYSTEM_PROMPT = [
    'ë‹¹ì‹ ì€ ë‘ AI ëª¨ë¸ì˜ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ìµœê³  í’ˆì§ˆì˜ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.',
    '',
    '## ì¢…í•© ì§€ì¹¨',
    '1. ê° ì‘ë‹µì—ì„œ ê°€ì¥ ê°•ë ¥í•˜ê³  ì •í™•í•œ í¬ì¸íŠ¸ë¥¼ ì‹ë³„í•˜ì„¸ìš”.',
    '2. ëª¨ìˆœë˜ëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´ ë” ì •í™•í•˜ê³  ìƒì„¸í•œ ìª½ì„ ì±„íƒí•˜ì„¸ìš”.',
    '3. ì–‘ìª½ì˜ ë³´ì™„ì  ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ê²°í•©í•˜ì„¸ìš”.',
    '4. ì½”ë“œ ë¸”ë¡, ë§ˆí¬ë‹¤ìš´ ì„œì‹, êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ì„¸ìš”.',
    '5. ì›ë³¸ ì§ˆë¬¸ê³¼ ë™ì¼í•œ ì–¸ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.',
    '',
    '## ì¶œë ¥ í˜•ì‹',
    'ìµœì¢… ì¢…í•© ë‹µë³€ë§Œ ì¶œë ¥í•˜ì„¸ìš”. "ëª¨ë¸ Aì— ë”°ë¥´ë©´..." ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.',
].join('\n');

/**
 * Agent-to-Agent ë³‘ë ¬ ìƒì„± ì „ëµ
 *
 * Primaryì™€ Secondary ë‘ ëª¨ë¸ì— ë™ì‹œì— ìš”ì²­ì„ ë³´ë‚´ê³ ,
 * ì–‘ìª½ ì‘ë‹µì´ ëª¨ë‘ ì„±ê³µí•˜ë©´ Synthesizerê°€ ì¢…í•© ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
 * í•œìª½ë§Œ ì„±ê³µí•˜ë©´ ë‹¨ë… ì‘ë‹µì„, ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ì‹¤íŒ¨ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 *
 * @class A2AStrategy
 * @implements {ChatStrategy<A2AStrategyContext, A2AStrategyResult>}
 */
export class A2AStrategy implements ChatStrategy<A2AStrategyContext, A2AStrategyResult> {
    /**
     * A2A ë³‘ë ¬ ìƒì„±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
     *
     * ì‹¤í–‰ íë¦„:
     * 1. Primary + Secondary ëª¨ë¸ì— Promise.allSettledìœ¼ë¡œ ë³‘ë ¬ ìš”ì²­
     * 2. ì–‘ìª½ ëª¨ë‘ ì„±ê³µ â†’ Synthesizerê°€ ë‘ ì‘ë‹µì„ ì¢…í•©
     * 3. í•œìª½ë§Œ ì„±ê³µ â†’ ë‹¨ë… ì‘ë‹µ ì‚¬ìš©
     * 4. ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨ â†’ succeeded=false ë°˜í™˜
     *
     * @param context - A2A ì „ëµ ì»¨í…ìŠ¤íŠ¸ (ë©”ì‹œì§€, ì˜µì…˜, í† í° ì½œë°±)
     * @returns A2A ì‹¤í–‰ ê²°ê³¼ (ì‘ë‹µ í…ìŠ¤íŠ¸ + ì„±ê³µ ì—¬ë¶€)
     * @throws {Error} abortSignalì— ì˜í•´ ì¤‘ë‹¨ëœ ê²½ìš° 'ABORTED' ì—ëŸ¬
     */
    async execute(context: A2AStrategyContext): Promise<A2AStrategyResult> {
        const startTime = Date.now();
        const models = resolveA2AModels(context.queryType);

        const clientA = new OllamaClient({ model: models.primary });
        const clientB = new OllamaClient({ model: models.secondary });

        logger.info(`ğŸ”€ A2A ë³‘ë ¬ ìš”ì²­ (queryType=${context.queryType ?? 'default'}): ${models.primary} + ${models.secondary}`);

        // ë‘ ëª¨ë¸ì— ë™ì‹œì— ìš”ì²­ (í•œìª½ì´ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ ìª½ ê²°ê³¼ë¥¼ í™œìš©)
        const [resultA, resultB] = await Promise.allSettled([
            clientA.chat(context.messages, context.chatOptions),
            clientB.chat(context.messages, context.chatOptions),
        ]);

        if (context.abortSignal?.aborted) {
            throw new Error('ABORTED');
        }

        // ê° ëª¨ë¸ì˜ ì‘ë‹µ ì¶”ì¶œ (ì‹¤íŒ¨í•œ ëª¨ë¸ì€ null)
        const responseA = resultA.status === 'fulfilled' ? resultA.value.content : null;
        const responseB = resultB.status === 'fulfilled' ? resultB.value.content : null;
        const durationParallel = Date.now() - startTime;

        logger.info(`ğŸ”€ A2A ë³‘ë ¬ ì™„ë£Œ (${durationParallel}ms): ` +
            `${models.primary}=${resultA.status}, ${models.secondary}=${resultB.status}`);

        // ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨: succeeded=falseë¥¼ ë°˜í™˜í•˜ì—¬ AgentLoop í´ë°± íŠ¸ë¦¬ê±°
        if (!responseA && !responseB) {
            logger.warn('âš ï¸ A2A ì–‘ìª½ ëª¨ë‘ ì‹¤íŒ¨');
            if (resultA.status === 'rejected') logger.warn(`  ${models.primary}: ${resultA.reason}`);
            if (resultB.status === 'rejected') logger.warn(`  ${models.secondary}: ${resultB.reason}`);
            return { response: '', succeeded: false };
        }

        // í•œìª½ë§Œ ì„±ê³µ: ì„±ê³µí•œ ëª¨ë¸ì˜ ì‘ë‹µì„ ë‹¨ë… ì‚¬ìš©
        if (!responseA || !responseB) {
            const singleResponse = (responseA || responseB) as string;
            const succeededModel = responseA ? models.primary : models.secondary;
            logger.info(`ğŸ”€ A2A ë‹¨ì¼ ì‘ë‹µ ì‚¬ìš©: ${succeededModel}`);

            const header = `> ğŸ¤– *${succeededModel} ë‹¨ë… ì‘ë‹µ*\n\n`;
            for (const char of header) {
                context.onToken(char);
            }
            for (const char of singleResponse) {
                context.onToken(char);
            }

            return {
                response: header + singleResponse,
                succeeded: true,
            };
        }

        // ì–‘ìª½ ëª¨ë‘ ì„±ê³µ: Synthesizer ëª¨ë¸ì´ ë‘ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
        logger.info(`ğŸ”€ A2A ì¢…í•© í•©ì„± ì‹œì‘ (synthesizer: ${models.synthesizer})`);

        // ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë©”ì‹œì§€ ì´ë ¥ì—ì„œ ì—­ìˆœ íƒìƒ‰í•˜ì—¬ ì¶”ì¶œ
        const userMessage = [...context.messages].reverse().find((m) => m.role === 'user')?.content || '';

        const synthesisUserMessage = [
            '## ì›ë³¸ ì§ˆë¬¸',
            userMessage,
            '',
            `## Response A (${models.primary})`,
            responseA,
            '',
            `## Response B (${models.secondary})`,
            responseB,
            '',
            'ìœ„ ë‘ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ìµœê³  í’ˆì§ˆì˜ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.',
        ].join('\n');

        const synthesizerClient = new OllamaClient({ model: models.synthesizer });
        let fullSynthesis = '';

        const header = `> ğŸ”€ *${models.primary} + ${models.secondary} A2A ì¢…í•© ë‹µë³€*\n\n`;
        for (const char of header) {
            context.onToken(char);
        }

        await synthesizerClient.chat(
            [
                { role: 'system', content: A2A_SYNTHESIS_SYSTEM_PROMPT },
                { role: 'user', content: synthesisUserMessage },
            ],
            { temperature: 0.3 },
            (token) => {
                fullSynthesis += token;
                context.onToken(token);
            }
        );

        const totalDuration = Date.now() - startTime;
        logger.info(`âœ… A2A ì¢…í•© ì™„ë£Œ: ë³‘ë ¬=${durationParallel}ms, í•©ì„±=${totalDuration - durationParallel}ms, ì´=${totalDuration}ms`);

        return {
            response: header + fullSynthesis,
            succeeded: true,
        };
    }
}
