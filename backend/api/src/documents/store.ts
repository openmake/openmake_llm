/**
 * Document Store
 * ì—…ë¡œë“œëœ ë¬¸ì„œ ì €ì¥ì†Œ
 * 
 * ğŸ”’ ë³´ì•ˆ ê°•í™”: TTL ê¸°ë°˜ ìë™ ì •ë¦¬ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
 * ğŸ“¦ PostgreSQL write-through cache: DBë¥¼ primary storeë¡œ ì‚¬ìš©í•˜ê³ 
 *    ì¸ë©”ëª¨ë¦¬ Mapì„ read cacheë¡œ ìœ ì§€í•˜ì—¬ ë¹ ë¥¸ ë™ê¸° ì½ê¸°ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
 */

import { DocumentResult } from './index';
import { getConfig } from '../config/env';
import { createLogger } from '../utils/logger';
import { getPool } from '../data/models/unified-database';
import type { Pool } from 'pg';

const logger = createLogger('DocumentStore');

// ë¬¸ì„œ TTL ì„¤ì • (ê¸°ë³¸: 1ì‹œê°„)
const DOCUMENT_TTL_MS = getConfig().documentTtlHours * 60 * 60 * 1000;
const MAX_DOCUMENTS = getConfig().maxUploadedDocuments;

interface StoredDocument {
    document: DocumentResult;
    createdAt: number;
    lastAccessedAt: number;
}

/**
 * DocumentStore ì¸í„°í˜ì´ìŠ¤ - Mapê³¼ í˜¸í™˜ë˜ëŠ” ìµœì†Œí•œì˜ ì¸í„°í˜ì´ìŠ¤
 */
export interface DocumentStore {
    get(key: string): DocumentResult | undefined;
    set(key: string, value: DocumentResult): this;
    delete(key: string): boolean;
    has(key: string): boolean;
    clear(): void;
    readonly size: number;
    forEach(callbackfn: (value: DocumentResult, key: string, map: DocumentStore) => void, thisArg?: unknown): void;
    entries(): IterableIterator<[string, DocumentResult]>;
    keys(): IterableIterator<string>;
    values(): IterableIterator<DocumentResult>;
    [Symbol.iterator](): IterableIterator<[string, DocumentResult]>;
}

// ============================================
// DB helper: fire-and-forget async operations
// All DB errors are caught and logged silently
// so the in-memory cache always remains functional.
// ============================================

/**
 * Safely obtain the pg Pool. Returns null when the pool is not yet
 * available (e.g. during early startup or in test environments).
 */
function safeGetPool(): Pool | null {
    try {
        return getPool();
    } catch {
        return null;
    }
}

function dbUpsertDocument(docId: string, document: DocumentResult, createdAt: number, lastAccessedAt: number): void {
    const pool = safeGetPool();
    if (!pool) return;

    const expiresAt = new Date(lastAccessedAt + DOCUMENT_TTL_MS).toISOString();
    const createdAtIso = new Date(createdAt).toISOString();
    const lastAccessedAtIso = new Date(lastAccessedAt).toISOString();

    pool.query(
        `INSERT INTO uploaded_documents (doc_id, document, created_at, last_accessed_at, expires_at)
         VALUES ($1, $2, $3, $4, $5)
         ON CONFLICT (doc_id) DO UPDATE SET
             document = EXCLUDED.document,
             last_accessed_at = EXCLUDED.last_accessed_at,
             expires_at = EXCLUDED.expires_at`,
        [docId, JSON.stringify(document), createdAtIso, lastAccessedAtIso, expiresAt]
    ).catch(err => {
        logger.warn(`[DocumentStore/DB] upsert failed for ${docId}: ${err instanceof Error ? err.message : String(err)}`);
    });
}

function dbUpdateLastAccessed(docId: string, lastAccessedAt: number): void {
    const pool = safeGetPool();
    if (!pool) return;

    const lastAccessedAtIso = new Date(lastAccessedAt).toISOString();
    const expiresAt = new Date(lastAccessedAt + DOCUMENT_TTL_MS).toISOString();

    pool.query(
        `UPDATE uploaded_documents SET last_accessed_at = $1, expires_at = $2 WHERE doc_id = $3`,
        [lastAccessedAtIso, expiresAt, docId]
    ).catch(err => {
        logger.warn(`[DocumentStore/DB] update last_accessed failed for ${docId}: ${err instanceof Error ? err.message : String(err)}`);
    });
}

function dbDeleteDocument(docId: string): void {
    const pool = safeGetPool();
    if (!pool) return;

    pool.query(`DELETE FROM uploaded_documents WHERE doc_id = $1`, [docId]).catch(err => {
        logger.warn(`[DocumentStore/DB] delete failed for ${docId}: ${err instanceof Error ? err.message : String(err)}`);
    });
}

function dbDeleteAllDocuments(): void {
    const pool = safeGetPool();
    if (!pool) return;

    pool.query(`DELETE FROM uploaded_documents`).catch(err => {
        logger.warn(`[DocumentStore/DB] clear failed: ${err instanceof Error ? err.message : String(err)}`);
    });
}

function dbCleanupExpired(): void {
    const pool = safeGetPool();
    if (!pool) return;

    pool.query(`DELETE FROM uploaded_documents WHERE expires_at < NOW()`).catch(err => {
        logger.warn(`[DocumentStore/DB] cleanup expired failed: ${err instanceof Error ? err.message : String(err)}`);
    });
}

/**
 * Async DB read â€” warms the in-memory cache on cache miss.
 * Called fire-and-forget from get(); the current get() returns undefined,
 * but the next get() will find the document in cache.
 */
function dbWarmCache(docId: string, cache: Map<string, StoredDocument>): void {
    const pool = safeGetPool();
    if (!pool) return;

    pool.query(
        `SELECT document, created_at, last_accessed_at FROM uploaded_documents WHERE doc_id = $1 AND expires_at > NOW()`,
        [docId]
    ).then(result => {
        if (result.rows.length === 0) return;
        const row = result.rows[0] as { document: DocumentResult; created_at: string; last_accessed_at: string };

        // Only warm if still absent from cache (avoid overwriting a fresher entry)
        if (!cache.has(docId)) {
            const doc: DocumentResult = typeof row.document === 'string'
                ? JSON.parse(row.document) as DocumentResult
                : row.document;
            cache.set(docId, {
                document: doc,
                createdAt: new Date(row.created_at).getTime(),
                lastAccessedAt: new Date(row.last_accessed_at).getTime()
            });
            logger.info(`[DocumentStore/DB] cache warmed for ${docId}`);
        }
    }).catch(err => {
        logger.warn(`[DocumentStore/DB] warm cache failed for ${docId}: ${err instanceof Error ? err.message : String(err)}`);
    });
}

/**
 * ğŸ”’ TTL ê¸°ë°˜ Document Map
 * Mapê³¼ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ë©° TTL ê¸°ë°˜ ìë™ ì •ë¦¬ ê¸°ëŠ¥ í¬í•¨
 * 
 * PostgreSQL write-through cache:
 * - set(): ìºì‹œ + DB ë™ì‹œ ê¸°ë¡
 * - get(): ìºì‹œ ìš°ì„ , ë¯¸ìŠ¤ ì‹œ ë¹„ë™ê¸° DB ì¡°íšŒë¡œ ìºì‹œ ì›Œë°
 * - delete(): ìºì‹œ + DB ë™ì‹œ ì‚­ì œ
 * - cleanupExpired(): ìºì‹œ + DB ë™ì‹œ ì •ë¦¬
 * 
 * Note: Mapì„ ì§ì ‘ ìƒì†í•˜ì§€ ì•Šê³  ë˜í¼ íŒ¨í„´ ì‚¬ìš© (Node.js 25+ TypeScript í˜¸í™˜ì„±)
 */
class TTLDocumentMap implements DocumentStore {
    private store: Map<string, StoredDocument> = new Map();
    private cleanupTimer: ReturnType<typeof setInterval>;

    constructor() {
        // ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ (10ë¶„ë§ˆë‹¤ ì‹¤í–‰)
        this.cleanupTimer = setInterval(() => this.cleanupExpired(), 10 * 60 * 1000);
    }

    /**
     * ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ (ì„œë²„ ì¢…ë£Œ ì‹œ)
     */
    dispose(): void {
        clearInterval(this.cleanupTimer);
    }

    /**
     * ğŸ”’ ë§Œë£Œëœ ë¬¸ì„œ ì •ë¦¬ (ìºì‹œ + DB)
     */
    private cleanupExpired(): void {
        const now = Date.now();
        let cleanedCount = 0;
        
        for (const [id, stored] of this.store.entries()) {
            if (now - stored.lastAccessedAt > DOCUMENT_TTL_MS) {
                this.store.delete(id);
                cleanedCount++;
            }
        }
        
        // DBì—ì„œë„ ë§Œë£Œëœ í–‰ ì‚­ì œ (fire-and-forget)
        dbCleanupExpired();
        
        if (cleanedCount > 0) {
            logger.info(`[DocumentStore] ë§Œë£Œëœ ë¬¸ì„œ ${cleanedCount}ê°œ ì •ë¦¬ë¨ (í˜„ì¬ ${this.store.size}ê°œ)`);
        }
    }

    /**
     * ğŸ”’ ìµœëŒ€ ê°œìˆ˜ ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ ë¬¸ì„œ ì œê±° (LRU ë°©ì‹)
     */
    private enforceMaxDocuments(): void {
        if (this.store.size <= MAX_DOCUMENTS) return;
        
        const entries = Array.from(this.store.entries())
            .sort((a, b) => a[1].lastAccessedAt - b[1].lastAccessedAt);
        
        const toRemove = entries.slice(0, this.store.size - MAX_DOCUMENTS);
        for (const [id] of toRemove) {
            this.store.delete(id);
            dbDeleteDocument(id);
        }
        
        logger.info(`[DocumentStore] ìš©ëŸ‰ ì´ˆê³¼ë¡œ ${toRemove.length}ê°œ ë¬¸ì„œ ì œê±°ë¨`);
    }

    // Map í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    get(key: string): DocumentResult | undefined {
        const stored = this.store.get(key);
        if (!stored) {
            // Cache miss â€” fire async DB read to warm cache for next call
            dbWarmCache(key, this.store);
            return undefined;
        }
        
        // ì ‘ê·¼ ì‹œê°„ ê°±ì‹  (LRU)
        stored.lastAccessedAt = Date.now();
        // DBì—ë„ ì ‘ê·¼ ì‹œê°„ ê°±ì‹  (fire-and-forget)
        dbUpdateLastAccessed(key, stored.lastAccessedAt);
        return stored.document;
    }

    set(key: string, value: DocumentResult): this {
        const now = Date.now();
        this.store.set(key, {
            document: value,
            createdAt: now,
            lastAccessedAt: now
        });
        
        // DBì—ë„ ê¸°ë¡ (fire-and-forget write-through)
        dbUpsertDocument(key, value, now, now);
        
        this.enforceMaxDocuments();
        logger.info(`[DocumentStore] ë¬¸ì„œ ì €ì¥: ${key} (ì´ ${this.store.size}ê°œ)`);
        return this;
    }

    delete(key: string): boolean {
        const result = this.store.delete(key);
        // DBì—ì„œë„ ì‚­ì œ (fire-and-forget)
        dbDeleteDocument(key);
        return result;
    }

    has(key: string): boolean {
        return this.store.has(key);
    }

    clear(): void {
        this.store.clear();
        // DBì—ì„œë„ ì „ì²´ ì‚­ì œ (fire-and-forget)
        dbDeleteAllDocuments();
    }

    get size(): number {
        return this.store.size;
    }

    forEach(callbackfn: (value: DocumentResult, key: string, map: DocumentStore) => void, thisArg?: unknown): void {
        for (const [key, stored] of this.store.entries()) {
            callbackfn.call(thisArg, stored.document, key, this);
        }
    }

    entries(): IterableIterator<[string, DocumentResult]> {
        const self = this;
        return (function* () {
            for (const [key, stored] of self.store.entries()) {
                yield [key, stored.document] as [string, DocumentResult];
            }
        })();
    }

    keys(): IterableIterator<string> {
        return this.store.keys();
    }

    values(): IterableIterator<DocumentResult> {
        const self = this;
        return (function* () {
            for (const stored of self.store.values()) {
                yield stored.document;
            }
        })();
    }

    [Symbol.iterator](): IterableIterator<[string, DocumentResult]> {
        return this.entries();
    }

    get [Symbol.toStringTag](): string {
        return 'TTLDocumentMap';
    }

    // ğŸ”’ ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    getStats(): { total: number; oldestAgeMinutes: number; newestAgeMinutes: number; ttlHours: number } {
        if (this.store.size === 0) {
            return { total: 0, oldestAgeMinutes: 0, newestAgeMinutes: 0, ttlHours: DOCUMENT_TTL_MS / 1000 / 60 / 60 };
        }
        
        const now = Date.now();
        let oldest = now;
        let newest = 0;
        
        for (const stored of this.store.values()) {
            if (stored.createdAt < oldest) oldest = stored.createdAt;
            if (stored.createdAt > newest) newest = stored.createdAt;
        }
        
        return {
            total: this.store.size,
            oldestAgeMinutes: Math.round((now - oldest) / 1000 / 60),
            newestAgeMinutes: Math.round((now - newest) / 1000 / 60),
            ttlHours: DOCUMENT_TTL_MS / 1000 / 60 / 60
        };
    }
}

// í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œ ì €ì¥ (ì‹±ê¸€í†¤)
export const uploadedDocuments: DocumentStore = new TTLDocumentMap();
